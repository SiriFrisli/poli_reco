{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script document the framework used in the paper \"Multimodal Critique of Authority â€“ Humor as a Function of Dissent during the COVID-19 Pandemic\" (paper in progress). \n",
    "\n",
    "The framework in question can be summarized as following:\n",
    "\n",
    "### Step 1: Contrastive image-text loss for representation learning\n",
    "### Step 2: Hybrid-modal attention for cross-modal fusion\n",
    "### Step 3: Humor classification\n",
    "### Step 4: Mutual learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer, CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load M-CLIP model, this will be used for the texts embeddings\n",
    "model_name = \"M-CLIP/XLM-Roberta-Large-Vit-B-32\"\n",
    "text_model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load CLIP model for image embeddings\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, otherwise fallback to CPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text):\n",
    "    \"\"\"Returns text embedding using M-CLIP.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        text_features = text_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Returns image embedding using CLIP.\"\"\"\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image)\n",
    "    return image_features\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"Contrastive loss for aligning text and image embeddings.\"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        batch_size = image_embeddings.shape[0]\n",
    "        similarities = torch.matmul(image_embeddings, text_embeddings.T) / self.temperature\n",
    "        labels = torch.arange(batch_size).to(similarities.device)\n",
    "        loss = (self.loss_fn(similarities, labels) + self.loss_fn(similarities.T, labels)) / 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop\n",
    "def train_contrastive_model(image_paths, texts, epochs=10, lr=1e-4):\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    contrastive_loss = ContrastiveLoss().to(device)\n",
    "    optimizer = optim.Adam(text_model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Extract embeddings\n",
    "        text_embeddings = torch.cat([get_text_embedding(t) for t in texts]).to(device)\n",
    "        image_embeddings = torch.cat([get_image_embedding(img) for img in image_paths]).to(device)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = contrastive_loss(image_embeddings, text_embeddings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Hybrid-modal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
